{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A basic introduction to Data Science\n",
    "**Disclaimer:** This is not a full introduction and unashamedly focusses on the areas I'm most interested in (optimisation). We are going to spend a small amount of time looking at howto extract data contained in text files. The focus is then on giving you an introduction to how some of the machine learning algorithms work under the hood (rather than just calling the existing implementations from a library). Overall this is very basic though, with a thorough treatment requiring at least a semester course. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario\n",
    "In this exerercise we are going to work through a fairly common scenario. After developing a couple of algorithms for solving a particular problem (or implementing some \"standard\" algorithms as well as your shiny new and hopefully improved algorithm), you test the algorithm on some instances and want to know which algorithm is \"best\"? Or which algorithm is best for what type of data? \n",
    "\n",
    "Here we are going to go through this type of analysis by focussing on alternative linear programming algorithms. So here are the elements that go into the analysis:\n",
    "* The problem to be solved:  $\\min c^T x$ subject to $A x = b,\\ x \\ge 0$ where $A$ is a $m\\times n$ matrix, $x$ is a vector of decision variables and $b$ and $c$ are appropriate constant vectors. In practice we normally allow some variants (for example inequality constraints, arbitrary lower and upper bounds on the variables). \n",
    "* Algorithms: Three standard algorithms, *primal* simplex, *dual* simplex, and the *barrier* method, as implemented by the CPLEX solver library\n",
    "* Instances: The [MIPLIB2010](http://miplib2010.zib.de/) \"standard\" test instances. The data sets are actually for mixed integer problems (where some variables are required to be binary or general integers), but for these tests we will throw away the binary requirements. There are 87 instances in this data set\n",
    "\n",
    "To make things easy you will not be required to re-run all of the tests but are given the results from running each algorithm on each instance.\n",
    "\n",
    "## Getting the data\n",
    "In an ideal world the data would already be sitting in a database or simple CSV file for you to read. However this may not always be possible. It is not uncommon for the data to be sitting in text files (perhaps the log files produced as the algorithms are run). Since such text files can get quite large, it makes sense to compress these. \n",
    "\n",
    "Your first task then is to obtain the data and display a few lines of one of the log files to see what these look like.\n",
    "That data can be found as a zip file at http://users.monash.edu/~andrease/Files/Other/cplexLP.zip. You could download this, unzip it and look at it - but we want to do things with python here. The basic library functions you need are\n",
    "* `urllib` using the `urllib.request.urlopen(\"name.of.url/to-load\")` function. Basic usage information is provided in this [how-to](https://docs.python.org/3.6/howto/urllib2.html?highlight=get%20url). It is probably easiest to just read this data and write it to file.\n",
    "* `zipfile` module provides methods for reading (and writing) zip archives. The basic usage is to create `zip=ZipFile(<file>,'r')` archive, then use `zip.namelist()` to inspect the contents and `zip.open(name,'r')` to open the named file. Complete documentation is [here](https://docs.python.org/3/library/zipfile.html)\n",
    "* Note that all log files are ASCII. To convert a binary (ASCII) string to the standard python unicode string use `.decode(\"utf-8\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "519756"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add your code here to get http://users.monash.edu/~andrease/Files/Other/cplexLP.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add code here to read one of the files\n",
    "# Note: if the output is too long you may want to manually \n",
    "#       toggle scrolling under `Cell->Current Outputs` in the notebook menu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Features\n",
    "For each instance we want to extract the following information:\n",
    "* The name (identifier) for the data set. This is encoded in the name of each file in the Zip archive (after the cplexLP/) or in the log file itself as part of the \"Problem name\"\n",
    "* Characteristics or *features* of the instance to be solved. The log file contains some \"statistics\" of the problem such as the number of constraints and variables, number of non-zero coefficients, etc. See the CPLEX documentation on [displaying problem statistics](https://www.ibm.com/support/knowledgecenter/SSSA5P_12.8.0/ilog.odms.cplex.help/CPLEX/GettingStarted/topics/tutorials/InteractiveOptimizer/displayingProbStats.html) for more informaiton\n",
    "* The solution algorithm used: this is again shown in the \"Problem name\" or can be seen as an integer in the log file as \"method for linear optimisation\" (1=primal, 2=dual, 4=barrier)\n",
    "* The time required using the solution algorithm: You could take the solution time in seconds, but for the analysis here using the \"Deterministic time\" in \"ticks\" is likely to be more useful as a way of assessing which algorithm is more efficient. Having said that, you may also want to extract the time in seconds. See this brief [discussion on ticks](http://www.thequestforoptimality.com/deterministic-behavior-of-cplex-ticks-or-seconds/) or CPLEX [documentation](https://www.ibm.com/support/knowledgecenter/SSSA5P_12.8.0/ilog.odms.cplex.help/CPLEX/Parameters/topics/DetTiLim.html) for a little more information about what these mysterious ticks mean\n",
    "\n",
    "In particular from the section of the log files that looks like this:\n",
    "```\n",
    "Objective sense      : Minimize\n",
    "Variables            :   18380  [Fix: 7282,  Box: 11098]\n",
    "Objective nonzeros   :       2\n",
    "Linear constraints   :     576  [Less: 486,  Equal: 90]\n",
    "  Nonzeros           :  109706\n",
    "  RHS nonzeros       :      30\n",
    "\n",
    "Variables            : Min LB: 0.000000         Max UB: 212.0000       \n",
    "Objective nonzeros   : Min   : 51.00000         Max   : 100.0000       \n",
    "Linear constraints   :\n",
    "  Nonzeros           : Min   : 1.000000         Max   : 224.0000       \n",
    "  RHS nonzeros       : Min   : 1.000000         Max   : 1.000000     \n",
    "\n",
    "...lines deleted...\n",
    "\n",
    "Deterministic time = 296.40 ticks  (956.13 ticks/sec)\n",
    "```\n",
    "We want to extract fields like the count of number of variables, the number of non-zeros or range of coefficient values.\n",
    "\n",
    "### Python functions and libraries to use\n",
    "\n",
    "When dealing with text files, regular expressions are your friend. They form a mini-language of their own for expressing string patterns. Regular expressions are not unique to python, with relatively similar syntax used in a variety of tools (particularly in linux/unix). The [regular expression documentation](https://docs.python.org/3/library/re.html) gives all the details, but to get you started the string `r\"^[abc]\\d*\"` would match any line that \n",
    "   * is at the start of a line (as incdicated by the `^`)\n",
    "   * has one of the characters in the set {'a','b','c'} (written as \"[abc]\") first\n",
    "   * is followed by zero or more digits \"\\d\" = digit, * means \"zero or more\", you could use \"+\" for \"one or more\"\n",
    "   * the string is written as a raw string (`r\"\"`) to make sure that \"\\\" is not interpreted as an escape character - since regular expressions make frequent use of the backslash\n",
    "   * it is also particularly useful and easy to mark a substring as a _group_. For example `r\"([xyz]+)\\1\"` means that we have a group consisting of one or more characters in {x,y,z} followed by the same group (group 1) repeating a second time. \n",
    "   * `import re` to get the library, then use `re.search(pattern,string)` to find a pattern in the given string, `re.search(pattern,string).group(1)` would give the substring of the first group in the time the pattern was matched (if the pattern was found at all).\n",
    "\n",
    "#### Regular expression testing\n",
    "To test your skills at writing regular expressions, try the following exercises:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4: Some;Australia;Capital;Figure\n",
      "3: enumerate;nested;array\n",
      "7: 15.3;-100;1.2e-03;NaN;1;4;+2E+6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[15.3, -100.0, 0.0012, nan, 1.0, 4.0, 2000000.0]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def testre(regex,string):\n",
    "    \"Simple test function to print all matches of a regular expression in a string\"\n",
    "    matches = re.findall(regex,string)\n",
    "    print(\"%d:\"%len(matches),\";\".join(matches))\n",
    "    return matches\n",
    "          \n",
    "testre(__INSERT_REGEX_HERE__, # insert regular expressions to find any capitalised words with 2 letters or more\n",
    "    \"\"\"Some words, like Australia I know, have Capital letters, see Figure 3 for more\"\"\"\n",
    "    )# should find 4: Some;Australia;Capital;Figure\n",
    "testre(__INSERT_REGEX_HERE__, # outer LaTeX environments: \\begin{somename}up to\\end{somename}\n",
    "      r\"\"\"\\begin{enumerate}\\item\n",
    "      \\end{enumerate}\\begin{nested}\\begin{env}\\end{env}\\end{nested}\n",
    "      \\begin{test}\\end{fail}  \\begin{array}1 & 2\\\\3 & 4 \\\\end{array}\n",
    "      \"\"\") # should find 3: enumerate;nested;array\n",
    "[float(f) for f in \n",
    " testre(__INSERT_REGEX_HERE__, \n",
    "        # find floating point numbers (including integers and scientific notation)\n",
    "      \"15.3 - -100 1.2e-03  NaN 1ee4 +2E+6 \")  # 7: 15.3;-100;1.2e-03;NaN;1;4;+2E+6\n",
    "] # output 15.3, -100.0, 0.0012, nan, 1.0, 4.0, 2000000.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting the data\n",
    "\n",
    "Have a go at extracting some/all of the data. Store the data in a pandas `DataFrame` object. See the [10 minute introduction](http://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html) to Pandas if you have not come across this before. Essentially it is a \"table like\" data structure that makes it easy to manipulate data. \n",
    "\n",
    "We want to extract the following fields relating to Variables (Var), Constraints (Con) or the Objective function (Obj):  VarCnt, VarFix,VarBox, ObjNZ,ConNZ, ConLess, ConEqual, ConNZ, RhsNZ, VarMin,VarMax,ObjMin,ObjMax,ConMin,ConMax,RhsMin,RhsMax.\n",
    "We also want the performance of the algorithm: PrimalT or DualT or BarrierT for the ticks.\n",
    "\n",
    "For this exercise there is no need to get all of the fields that might be interesting them all (as getting them all can be a bit fiddly). To get a complete data set we will load a DataFrame of results extracted from these files below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re,pandas\n",
    "import numpy as np\n",
    "\n",
    "###### insert your code here ###############\n",
    "\n",
    "data # finish by displaying the DataFrame obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VarCnt</th>\n",
       "      <th>VarFix</th>\n",
       "      <th>VarBox</th>\n",
       "      <th>ObjNZ</th>\n",
       "      <th>ConCnt</th>\n",
       "      <th>ConLess</th>\n",
       "      <th>ConEqual</th>\n",
       "      <th>ConNZ</th>\n",
       "      <th>RhzNZ</th>\n",
       "      <th>VarMin</th>\n",
       "      <th>...</th>\n",
       "      <th>ConMax</th>\n",
       "      <th>RhsMin</th>\n",
       "      <th>RhsMax</th>\n",
       "      <th>PrimalT</th>\n",
       "      <th>DualT</th>\n",
       "      <th>BarrierT</th>\n",
       "      <th>ConGreater</th>\n",
       "      <th>VarNneg</th>\n",
       "      <th>VarOther</th>\n",
       "      <th>VarFree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [VarCnt, VarFix, VarBox, ObjNZ, ConCnt, ConLess, ConEqual, ConNZ, RhzNZ, VarMin, VarMax, ObjMin, ObjMax, ConMin, ConMax, RhsMin, RhsMax, PrimalT, DualT, BarrierT, ConGreater, VarNneg, VarOther, VarFree]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 24 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's just check for missing values\n",
    "data[np.logical_or.reduce(data.isnull(),axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting with just the data\n",
    "To make things easy, here is a way to just start with the data you should have by now (you can also skip the above and just start from here).\n",
    "As a reminder on using data frames, this [pandas selection tutorial](https://medium.com/dunder-data/selecting-subsets-of-data-in-pandas-6fcd0170be9c) provides a good summary on how to access rows,columns etc \n",
    "\n",
    "Also to make things easy there is a version of the data that has\n",
    "* All timing (tick) information for the different algoritms removed\n",
    "* Replace the min/max value of coefficients with the magnituded (log) of the range\n",
    "* Scaled all columns to have entries in the range $[-1,1]$ (to have similar magnitudes)\n",
    "\n",
    "You can load the raw data (including 'PrimalT', 'DualT' and 'BarrierT' performance fields) and the preprocessed features using the commands below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reload using: data = pandas.read_json(open(\"cplexLP.json\",\"r\"))\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "from urllib.request import urlopen\n",
    "data = pandas.read_json(urlopen(\"http://users.monash.edu/~andrease/Files/Other/cplexLP.json\"))\n",
    "features = pandas.read_json(urlopen(\"http://users.monash.edu/~andrease/Files/Other/features.json\"))\n",
    "features.head(5) # show first few rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising our data\n",
    "The set of features is fairly high dimensional. How do we make sense of this?\n",
    "Let's try to do a plot. For this we need to reduce the data to two dimensions. Enter PCA (Principal Component Analysis).\n",
    "For most of the \"machine learning\" type algorithms in this workshop we will use the [scikit-learn](https://scikit-learn.org/stable/documentation.html) set of libraries. This includes a [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) method. However so we can see more easily what is going on here, let's do it directly using the numpy matrix routines.\n",
    "\n",
    "Useful functions:\n",
    "* `data.values` gives the whole frame as a matrix. \n",
    "* We need to form the covariance matrix (subtract the column mean, multiply by the transpose and divide by n-1): $cov= \\frac{1}{n-1}(X-I\\bar x)^T (X-I\\bar x)$. For this you need `np.mean` which takes an argument `axis=d` to take the mean in the d'th dimension, and `X.T` is the transpose of numpy matrix `X`. Or just call `numpy.cov` - but be careful not to get features & observations (columns and rows) mixed up\n",
    "* `np.linalg.eig(X)` returns the (eigenvalues, eigenvectors) as vector/matrix respectively. \n",
    "* We only want to keep the two largest eigenvectors and plot the projection onto these two axes\n",
    "* I'm assuming you are using matplotlib (`matplotlib.pyplot.scatter`) but you can use anything else for plotting\n",
    "\n",
    "Question: Is the scaling of the different attributes proposed above sufficient? Shoud we do something more radical?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "## insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For alternative, more complicated ways to map data down to 2 dimensions there are plenty of other tools in sklearn library (look at [Manifold Learning](https://scikit-learn.org/stable/modules/manifold.html)). Looking at underlying algoirthms is well beyond the scope of this tutorial.  However you may want to have have a go at a few of these such as `TSNE` or `MDS`. Note that some of these manifold learning methods are randomised so won't return the same result each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE, MDS,LocallyLinearEmbedding,SpectralEmbedding\n",
    "## you may want to give this a go ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised learning - clustering\n",
    "In unsupervised learning we are looking for some structure without having any real idea of what is \"correct\". The simplest case of unsupervised learning is clustering - grouping data into clusters of points that are close together. \n",
    "\n",
    "The simplest way to do this is to apply the `cluster.KMeans` model from the `scklearn` library. See this [tutorial](https://scikit-learn.org/stable/tutorial/statistical_inference/unsupervised_learning.html) for more information.\n",
    "Try splitting the data (based on the features table) into 3 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cluster\n",
    "ncluster=3\n",
    "### insert your code here and plot the results ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing our own k-means algorithm\n",
    "The basic algorithm is very simple - so simple that we can write our own\n",
    "```\n",
    "Pick an initial assignment of points to clusters (or an initial set of centres of the clusters)\n",
    "Repeat until converged:\n",
    "    Assign each point to the nearest centre (euclidean distance)\n",
    "    Move the center of each cluster to the centroid (average of all points in the cluster)\n",
    "```\n",
    "\n",
    "This method is a heuristic: It is not guaranteed to give an optimal solution (the one with the least average distance of points to their centre) and it will converge to different solutions depending on the initial solution we start with.\n",
    "\n",
    "Some usuful numpy funtions\n",
    "* `np.random.randint(low,high,n)` returns array length n of integers in `range(low,high)`\n",
    "* `np.mean(matrix,axis=d)` = array of means (along either columns or rows depending on axis\n",
    "* `np.linalg.norm` = norm of an array\n",
    "* `np.argmin(matrix,axis=d)` = like mean iterates over elements along just one axis and returns the minimum index\n",
    "* `X[v > 0]` - returns a submatrix of X depending on which elements of `v` are positive. Whether `v` may be of length number of rows (to select whole rows) or same dimension as X (to select submatrix). Similarly `X[:,v>0]` would select submatrix based on columns where `v` is positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def kmeansHeur(data,p):\n",
    "    \"\"\"Given a n x m matrix with n points and m features \n",
    "    return list/np.array length n of labels (in range(p))  \n",
    "    \"\"\"\n",
    "    data = np.array(data) # just to make convert from DataFrame or other data type\n",
    "    ### insert your algorithm here #####\n",
    "    return lbl\n",
    "\n",
    "lblsHeur = kmeansHeur(features,ncluster) # fit model to data\n",
    "print(\"Points per cluster=\",[sum(1 for i in lblsHeur if i==k) for k in range(ncluster)])\n",
    "# do a scatter plot here using c=lbslHeur to get the dots coloured by cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This answer is probably not the same that you got wiht KMeans from sklearn. Which one is \"right\" or at least \"better\"? Need to formally define the objective function that we are trying to optimise. Essentially we are minimising the sum of squared norm distances:\n",
    "$$\\min_{c_k,C_k} \\sum_k \\sum_{i\\in C_k} ||c_k - x_i||^2$$\n",
    "Where $c_k$ and $C_k$ are the centre of cluster $k$ and the set of points in the cluster respectively. Each $x_i$ is a point of the data.\n",
    "\n",
    "Compute the objective function for both your solution and the KMeans solution. Which one is better? Is either of these optimal (and how would we know?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35.92617097573589, 44.328479351670445)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clusterObj(data,label):\n",
    "    \"Input: feature matrix (pts x features) and label for each feature.\"\n",
    "    data = np.array(data)\n",
    "    # compute the centre for each cluster and return the sum of squared distances\n",
    "clusterObj(features,k_means.labels_),clusterObj(features,lblsHeur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our method is a randomised heuristic and the result depends on the starting point. Rerun your `kmeansHeur` repeatedly (say 1000 times) and keep the best result. How does this compare to the result from a single run of `sklearn.cluster.KMeans` ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### insert your code here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning - Classification with Support Vector Machines\n",
    "We might want to figure out which algorithm works fastest for a given problem. That is, given a linear program and its characteristics, can we decide which algorithm to use to get the solution as fast as possible. Support vector machines are one model for such a classification task. It supposes that there is a _linear_ function of the factors that determines which of 2 classes a point belongs to. That is, that there exists a set of weights $w_i$ and constant $W$ such that\n",
    "$\\sum_i w_i f_i < W$ if one algorithm is faster and $\\sum_i w_i f_i > W$ when the other method is faster where $f_i$ are the features of the instance we are interested in solving. How do we decide what the $w_i$ (and $W$) should be? This is supervised learning where we are using existing _training data_ to fit the parameters. \n",
    "\n",
    "Let's start by trying to work out for which instances the primal vs dual simplex method is faster. We have `primalT` and `dualT` that tells us which is faster for the test data. So we want to find a vector $w$ and constant $W$ such that\n",
    "$\\sum_i w_i f_{ki} \\le W-1$ if `PrimalT` < `DualT` and $\\sum_i w_i f_{ki} \\ge  W+1$ otherwise. (Where $f_{ki}$ is the i-th feature of instance k.  Finding such a set of $w_i$ and $W$ is just a linear programming problem. Some things we need to consider is: The limit of 1 is just to ensure we get some minimum separation (we could pick any number as multiplying each row by a constant would not change the problem other than to increase the non-zero difference between the LHS and W\n",
    "* If the instances can be separated, we want to make the 2 hyperplanes (defined by the $\\pm1$ on the right hand side) as far apart as possible(so that we get a clean separation). The separation distance is $2/||w||$. So maximising the separation is equivalent to $\\min ||w||^2$ which gives us a quadratic program with linear constraints.\n",
    "* It may not be possible to separate the points cleanly into two groups with a single hyperplane. Hence we might want to penalise any point that is mis-classified, perhaps based on how far it is away from the hyperplane. Let $v_k$ be the distance that point $k$ is away from the hyperplane then we might want to $\\min ||w||^2 + \\sum_k v_k$ with $\\sum_i w_i f_{ki} \\le W-1+v_k$,\n",
    "* We can put the two objectives together - by noting that maximisation is the same as minimising the negative and placing greater priority on minimising violations:\n",
    "$\\min ||w||^2 + \\alpha \\sum_k v_k$ subject to $\\sum_i w_i f_{ki} \\le W+v_k-s$ for $k$ such that the primal solution is better (with correspoding $\\ge$ constraints for the other points). Here $\\alpha$ is an arbitrary weight that provides a trade-off between violations (misclassifications) and the distance separating the hyperplanes.\n",
    "* Some variants of this are possible. For example we could replace the$||w||^2$ term by $\\max_i |w_i|$ (for example) which can be written in a linear program, or even drop it entirely. When there are violations these are minimised  if $||w||$ is minimised so we may not need this.\n",
    "\n",
    "Hence for an initial experiment we want to set up an SVM training function that takes our features as input data together with the list instances for which `data['PrimalT'] < data['dualT']` (this python expression returns a boolean column). Let `I` be those instances and `IC` be the complement. The we want to solve the following linear program:\n",
    "$$\\min_{v,w,W} \\sum_{i\\in I\\cup IC} v_i$$\n",
    "$$s.t.\\ \\sum_{k\\in F} D_{if} w_f \\le W - 1 + v_i\\quad\\forall\\ i\\in I$$\n",
    "$$\\quad \\sum_{k\\in F} D_{if} w_f \\ge W + 1 - v_i\\quad\\forall\\ i\\in IC$$\n",
    "$$ v_i \\ge 0\\quad\\forall\\ i$$\n",
    "Here $D_{if}$ is the data for instance $i$ and feature $f$ out of the set of features $F$.\n",
    "\n",
    "How to set this LP up using the CPLEX solver:\n",
    "* `import docplex.mp.model as MP`\n",
    "* `mdl = MP.Model()` create a mathematical programming (optimisation) model \n",
    "* `x = mdl.binary_var_list(n)` create list of variables `x[0]`...`x[n-1]` that are all in {0,1} - not needed here\n",
    "* `x = mdl.continuous_var_list(n,lb=-mdl.infinity,ub=3*np.ones(n))` create list of variables $-\\infty < x[i] \\le 3$ for i=0,...,n-1\n",
    "* `mdl.add( mdl.sum(x[i] for i in range(n)) == 1)` add the constraint $\\sum_{i=0}^{n-1} x_i=1$\n",
    "* `mdl.minimize( x[0]+x[1]**2)` set the objective (linear + quadratic term). Note: CPLEX handles linear and quadratic programming problems but not more general non-linear optimisation problems\n",
    "* `CPXparam = mdl.context.cplex_parameters` access parameters - complete list of [CPLEX parameters](https://www.ibm.com/support/knowledgecenter/SSSA5P_12.8.0/ilog.odms.cplex.help/CPLEX/Parameters/topics/introListAlpha.html). It may be a good idea to set `CPXparam.timelimit=60` seconds (to stop it running too long if the problem is not set up correctly), `CPXparam.threads=1`. This model should run in no more than a second though.\n",
    "* `solution = mdl.solve(log_output=True)` does the optimisation and shows logs some information to the screen as it runs. `solution == None` if the solver fails to obtain a solution.\n",
    "* `mdl.get_solve_status()` gives the final solution status (as a string)\n",
    "* `solution.get_value(x[0]+x[1])` would return the final value of `x[0]+x[1]` in the solution. Alternatively you can also use `solution[x[2]]` to get the value of `x[2]` in the solution. (More informmation on the solution class see [SolveSolution](http://ibmdecisionoptimization.github.io/docplex-doc/mp/docplex.mp.solution.html#docplex.mp.solution.SolveSolution))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematical programming optimisation model\n",
    "import docplex.mp.model as MP\n",
    "\n",
    "def SVM(features,label):\n",
    "    \"\"\"features: the usual matrix of points x features\n",
    "    label: a list/array (length number of points) of bools/integers that\n",
    "           identifies the points to be separated from the rest\n",
    "    Returns: array/list length number of features+1 of w_i plus the constant W\"\"\"\n",
    "    # insert your code here and return the solution\n",
    "    return [solution.get_value(w[f]) for f in F]\n",
    "soln = SVM(features,data['PrimalT']<data['DualT'])\n",
    "print(\"Weights =\",soln)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can we do to improve the fit? How to deal with non-linear separations? Try adding additional \"features\" that capture the non-linear effects. For example we might expect that the effectiveness depends on the size or density of the constraint matrix (density = 'ConNZ' / ('VarCnt' x 'ConCnt')). Create an extended feature set and try again.\n",
    "The idea here is that if we have say a two dimensional set of features (x,y for each point) and we wanted to separate those points that are inside an elipse centered at the origin from those outside, then we can find such a separating elipse by including features $x^2$ and $y^2$ with the optimisation choosing some combination $w_x x^2 + w_y y^2 \\le W$ giving us an elipse while still solving a lienar problem (since the $x^2$ is just a constant coefficent for the variable $w_x$ in the optimisation).\n",
    "\n",
    "Try creating some additional features. Remember for `DataFrames` we can easily create new columns by doing arithmetic with whole columns. For example `features['eqSq'] = features['ConEqual']**2` would create a new column `eqSq` that contains the number of equality constraints (`ConEqual`) squared. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expFeat = features.copy() # expanded set of features\n",
    "### add features here ####\n",
    "expSoln = SVM(expFeat,data['PrimalT']<data['DualT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How well is our classifier doing?\n",
    "To analyse the performance we should think about:\n",
    "* Do we care about:\n",
    "    1. how far we are on the wrong side of the line? (this is what we are optimising at the moment)\n",
    "    2. how many instances are classified incorrectly?\n",
    "    3. how much extra compute time we would incur if we used the minimum?\n",
    "* How well it works for data it hasn't seen?\n",
    "    \n",
    "Below compute some alternative measures of the quality of the fit. Then address the second issue by using a random split of the instances to create 4 groups (of about 21 each). Then we used 3 parts as the \"training\" data to fit the SVM model, and the other group for testing to validate that the model is acutally OK on data that wasn't used for the training. By repeating this 4 times for each group we can compute a more accurate average performance of the approach on this type of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVMviolation(feat,w,lower=data['PrimalT'],higher=data['DualT']):\n",
    "    \"\"\"Input: feat = matrix of features, w - list/array of weights (length features + 1), \n",
    "     higher/lower = performance measure\n",
    "     Returns total violation of sum( w[i]*feat[i]) <= W or >= W depending on if lower[i] < or > higher[i]\"\"\"\n",
    "    ### insert your code here\n",
    "    \n",
    "def SVMextraT(feat,w,lower=data['PrimalT'],higher=data['DualT']):\n",
    "    \"\"\"Input: feat = matrix of features, w - list/array of weights (length features + 1), \n",
    "     higher/lower = performance measure  (if feat * w < W we run the 'lower' method)\n",
    "     Returns total extra time for running the slower algorithm\"\"\"\n",
    "    #### insert your code here \n",
    "    \n",
    "def SVMcount(feat,w,lower=data['PrimalT'],higher=data['DualT']):\n",
    "    \"\"\"Input: feat = matrix of features, w - list/array of weights (length features + 1), \n",
    "     higher/lower = performance measure  (if feat * w < W we run the 'lower' method)\n",
    "     Returns total extra time for running the slower algorithm\"\"\"\n",
    "    ### insert your code here\n",
    "\n",
    "    \n",
    "print(\"Results from training on whole data\")\n",
    "print(\"Effectiveness of solution: %.2f viol, %.2f ticks, %d misclassified\"%(\n",
    "    SVMviolation(features,soln),SVMextraT(features,soln),SVMcount(features,soln)))\n",
    "print(\"Expanded feature set soln: %.2f viol, %.2f ticks, %d misclassified\"%(\n",
    "    SVMviolation(expFeat,expSoln),SVMextraT(expFeat,expSoln),SVMcount(expFeat,expSoln)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now partition your data into 4 subsets of about 21 instances each. Take each subset in turn as the test data and use the remainder to train (fit) the SVM. How well, on average, does this approach work for data it hasn't been trained on?\n",
    "\n",
    "Note: while in general one might want to do the splitting randomly, this could lead to somewhat misleading results. The data sets are very variable in size & complexity, so it would make sense to try to split them a bit more systematically to ensure none of the groups only have very big/complex problems or only small trivial data sets. If you are feeling creative, make up a way to split the instances that deterministic or less random and more likely to be balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPXPARAM_Read_DataCheck                          1\n",
      "Tried aggregator 1 time.\n",
      "LP Presolve eliminated 2 rows and 18 columns.\n",
      "Aggregator did 1 substitutions.\n",
      "Reduced LP has 55 rows, 57 columns, and 745 nonzeros.\n",
      "Presolve time = 0.00 sec. (0.14 ticks)\n",
      "Initializing dual steep norms . . .\n",
      "\n",
      "Iteration log . . .\n",
      "Iteration:     1   Dual objective     =             0.000000\n",
      "Solved with status JobSolveStatus.OPTIMAL_SOLUTION\n",
      "Total violation =  25.846194539417073 7/3 points misclassified out of 58\n",
      "Separation =  0.0001707715384478243\n",
      "CPXPARAM_Read_DataCheck                          1\n",
      "Tried aggregator 1 time.\n",
      "LP Presolve eliminated 2 rows and 14 columns.\n",
      "Reduced LP has 68 rows, 74 columns, and 920 nonzeros.\n",
      "Presolve time = 0.00 sec. (0.13 ticks)\n",
      "Initializing dual steep norms . . .\n",
      "\n",
      "Iteration log . . .\n",
      "Iteration:     1   Dual objective     =             0.000000\n",
      "Iteration:    62   Dual objective     =            33.875482\n",
      "Solved with status JobSolveStatus.OPTIMAL_SOLUTION\n",
      "Total violation =  34.94454272579978 5/11 points misclassified out of 70\n",
      "Separation =  3.8695707264822075e-05\n",
      "CPXPARAM_Read_DataCheck                          1\n",
      "Tried aggregator 1 time.\n",
      "LP Presolve eliminated 0 rows and 13 columns.\n",
      "Reduced LP has 64 rows, 69 columns, and 889 nonzeros.\n",
      "Presolve time = 0.00 sec. (0.12 ticks)\n",
      "Initializing dual steep norms . . .\n",
      "\n",
      "Iteration log . . .\n",
      "Iteration:     1   Dual objective     =             0.000000\n",
      "Iteration:    62   Dual objective     =            37.870247\n",
      "Solved with status JobSolveStatus.OPTIMAL_SOLUTION\n",
      "Total violation =  38.05281703487499 5/13 points misclassified out of 64\n",
      "Separation =  0.001642740150009337\n",
      "CPXPARAM_Read_DataCheck                          1\n",
      "Tried aggregator 1 time.\n",
      "LP Presolve eliminated 0 rows and 11 columns.\n",
      "Reduced LP has 69 rows, 76 columns, and 959 nonzeros.\n",
      "Presolve time = 0.00 sec. (0.13 ticks)\n",
      "Initializing dual steep norms . . .\n",
      "\n",
      "Iteration log . . .\n",
      "Iteration:     1   Dual objective     =             0.000000\n",
      "Iteration:    62   Dual objective     =            35.678993\n",
      "Solved with status JobSolveStatus.OPTIMAL_SOLUTION\n",
      "Total violation =  35.904024755019826 3/14 points misclassified out of 69\n",
      "Separation =  0.007035393760542038\n",
      "Performance with 4 groups:1856.27 viol, 9805082.54 ticks, 48 misclassified\n",
      "CPXPARAM_Read_DataCheck                          1\n",
      "Tried aggregator 1 time.\n",
      "LP Presolve eliminated 0 rows and 18 columns.\n",
      "Reduced LP has 62 rows, 68 columns, and 1230 nonzeros.\n",
      "Presolve time = 0.00 sec. (0.17 ticks)\n",
      "Initializing dual steep norms . . .\n",
      "\n",
      "Iteration log . . .\n",
      "Iteration:     1   Dual objective     =             0.000000\n",
      "Perturbation started.\n",
      "Iteration:    51   Dual objective     =             0.000000\n",
      "Removing perturbation.\n",
      "Solved with status JobSolveStatus.OPTIMAL_SOLUTION\n",
      "Total violation =  16.6574843028596 2/6 points misclassified out of 62\n",
      "Separation =  0.0005100711480481556\n",
      "CPXPARAM_Read_DataCheck                          1\n",
      "Tried aggregator 1 time.\n",
      "LP Presolve eliminated 0 rows and 16 columns.\n",
      "Reduced LP has 61 rows, 69 columns, and 1202 nonzeros.\n",
      "Presolve time = 0.00 sec. (0.16 ticks)\n",
      "Initializing dual steep norms . . .\n",
      "\n",
      "Iteration log . . .\n",
      "Iteration:     1   Dual objective     =             0.000000\n",
      "Iteration:    62   Dual objective     =            22.943956\n",
      "Solved with status JobSolveStatus.OPTIMAL_SOLUTION\n",
      "Total violation =  23.44995181413229 6/6 points misclassified out of 61\n",
      "Separation =  0.0007552419195052046\n",
      "CPXPARAM_Read_DataCheck                          1\n",
      "Tried aggregator 1 time.\n",
      "LP Presolve eliminated 1 rows and 20 columns.\n",
      "Reduced LP has 67 rows, 72 columns, and 1301 nonzeros.\n",
      "Presolve time = 0.00 sec. (0.21 ticks)\n",
      "Initializing dual steep norms . . .\n",
      "\n",
      "Iteration log . . .\n",
      "Iteration:     1   Dual objective     =             0.000000\n",
      "Solved with status JobSolveStatus.OPTIMAL_SOLUTION\n",
      "Total violation =  21.538678238361175 4/2 points misclassified out of 68\n",
      "Separation =  1.1258324539094591e-05\n",
      "CPXPARAM_Read_DataCheck                          1\n",
      "Tried aggregator 1 time.\n",
      "LP Presolve eliminated 0 rows and 17 columns.\n",
      "Reduced LP has 70 rows, 77 columns, and 1385 nonzeros.\n",
      "Presolve time = 0.00 sec. (0.19 ticks)\n",
      "Initializing dual steep norms . . .\n",
      "\n",
      "Iteration log . . .\n",
      "Iteration:     1   Dual objective     =             0.000000\n",
      "Iteration:    62   Dual objective     =            16.821517\n",
      "Solved with status JobSolveStatus.OPTIMAL_SOLUTION\n",
      "Total violation =  18.917275053481482 5/1 points misclassified out of 70\n",
      "Separation =  0.0001448685861530269\n",
      "Performance with 4 groups:1125.87 viol, 9787073.67 ticks, 36 misclassified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1125.871757496142, 9787073.669999996, 36)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do things correctly using a subset of data for training only\n",
    "def SVMtest(feat,nGroup=4,method=SVM,lower=data['PrimalT'],higher=data['DualT']):\n",
    "    \"\"\"Split features feat into nGroup groups, train using method, evaluate using lower/higher\"\"\"\n",
    "    ### insert your code here\n",
    "    return (violation,extraT,count) # total performance across all 4 groups with the 3 performance measures\n",
    "\n",
    "print(\"Results when using original features: %.2f violation, %.2f ticks, %d misclassified\"%\n",
    "      SVMtest(features))\n",
    "print(\"Results when using expanded features: %.2f violation, %.2f ticks, %d misclassified\"%\n",
    "    SVMtest(expFeat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension exercises\n",
    "* We could modify our SVM approach to use an objective that better matches what we want to achieve (e.g. minimise additional computational effort from misclassified instances)\n",
    "* Use the sklearn builtin method `LinearSVC` to implement a support vector machine. Documentation is available [here](https://scikit-learn.org/stable/modules/svm.html) The basic approach is:\n",
    "```python\n",
    "from sklearn import svm\n",
    "mdl = svm.LinearSVC()\n",
    "mdl.fit(X,y) # X = training data, y is -1 when PrimalT < DualT and +1 otherwise\n",
    "mdl.predict(x) # predict outcome using test data x\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
